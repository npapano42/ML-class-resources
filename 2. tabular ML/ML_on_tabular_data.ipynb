{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Introduction\n",
        "\n",
        "Welcome to some real ML!\n",
        "\n",
        "We'll be using the titanic dataset to predict if a passenger survived this tragic maiden voyage\n",
        "\n",
        "For tools, we'll stick to scikit-learn to take advantage of their pre-built algorithms, and also cover a few subjects in preprocessing and data cleaning along the way\n"
      ],
      "metadata": {
        "id": "PRr5UWtwMnYL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ***REMINDER!***\n",
        "\n",
        "---\n",
        "\n",
        "Machine learning is defined with a fixed input and output.\n",
        "\n",
        "How would you fill in the statement below?\n",
        "\n",
        "\"Given \\_\\_\\_, can we predict \\_\\_\\_?\""
      ],
      "metadata": {
        "id": "NWnknIADOlRX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# The Dataset\n",
        "We'll be loading a CSV of 1309 passengers on the titanic. Official accounts place the passenger amount slightly above this, and the full list including crew is 2240 souls\n",
        "\n"
      ],
      "metadata": {
        "id": "56FFiZ3tgowQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Loading the dataset\n",
        "Loading a CSV into colab is quite easy.\n",
        "\n",
        "0. Take the CSV file and upload it into your google drive. If possible, upload it to the \"root\" of your google drive (not in any folders). If you don't upload it to the root, you'll need to update the location\n",
        "1. Click the folder button on the left below the variables to open up the files button.\n",
        "2. Connect the colab runtime to your google drive with the google drive folder button, and grant it permissions to view your drive\n",
        "3. Under `content/drive/MyDrive`, your full google drive will be visible. Find your file, and right click on it, then click \"Copy Path\". (note that if it's buried in some folders, find it in those folders)\n",
        "4. Paste path into the string variable `csv_loc`, then run the cell. If a table pops up, nice! Otherwise, you'll need to debug"
      ],
      "metadata": {
        "id": "1x2BLA0IQpXm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "csv_loc = \"\"\n",
        "\n",
        "df = pd.read_csv(csv_loc)\n",
        "\n",
        "df"
      ],
      "metadata": {
        "id": "gQ7D0_KZqL40"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## About the data\n",
        "---\n",
        "A data dictionary will sometimes be provided to explain the columns. This is especially helpful when columns can contain engineered features where the math behind the feature is important to understand. Or the column has a confusing name\n",
        "\n",
        "Here is the data dictionary for the columns on the dataset:\n",
        "\n",
        "* survived - Survival (0 = No; 1 = Yes)\n",
        "* pclass - Passenger Class (1 = 1st; 2 = 2nd; 3 = 3rd)\n",
        "* name - The passenger's name\n",
        "* sex - Their sex (male/female in this case)\n",
        "* age - Age\n",
        "* sibsp - Number of Siblings/Spouses Aboard\n",
        "* parch - Number of Parents/Children Aboard\n",
        "* ticket - Ticket Number\n",
        "* fare - Passenger Fare\n",
        "* cabin - Cabin\n",
        "* embarked - Port of Embarkation (C = Cherbourg; Q = Queenstown; S = Southampton)\n",
        "* boat - Lifeboat (if survived)\n",
        "* body - Body number (if did not survive and body was recovered)\n",
        "* home.dest - home and destination (if known)"
      ],
      "metadata": {
        "id": "QIa8oGK_is6M"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Exploration\n",
        "\n",
        "This is an extremely important first step in looking at a new dataset and should always come first when looking at data in general. The goal here is to see what we will be working with, and if we notice anything ahead of time. Think of this as doing your due-diligence, and it will help guide your next steps."
      ],
      "metadata": {
        "id": "xOA0tskKQhMr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Food for thought\n",
        "---\n",
        "What would you first want to know about the dataset?"
      ],
      "metadata": {
        "id": "MU5P7EwQRoxY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Null-checking\n",
        "\n",
        "Missing data can be a nightmare when it comes to data science. Null-checking every value isn't fun, so it is generally cleaned out of the dataset using a variety of methods.\n",
        "\n",
        "We'll get to data cleaning soon, so for now, let's see what's missing!\n",
        "\n"
      ],
      "metadata": {
        "id": "iFLC48GMR1j6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# This will tell us if values are missing by converting it to a boolean (true/false) value\n",
        "\n",
        "df.isnull()"
      ],
      "metadata": {
        "id": "VQBdEdbs_VMN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "That table isn't very readable, since it doesn't summarize anything. \n",
        "\n",
        "Looking through every single value in the table is a waste, so let's make the computer do it for us"
      ],
      "metadata": {
        "id": "7vRGbNGhFh4t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's figure out what is *actually* missing\n",
        "df.isnull().sum()"
      ],
      "metadata": {
        "id": "cySfeXZB_I0U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Yikes! That's a lot of missing data. Those columns may be a lot of work to clean up, but we'll come back to that down below.\n",
        "\n",
        "Lets also look into the data of some columns.\n",
        "\n",
        "First off, that home.dest column is interesting, but how can we check what's in it?\n",
        "\n",
        "### Do it yourself!\n",
        "\n",
        "If you run the cell below, you'll get just the one column from the data. Look around online and figure out if there's a function you can use to get only the unique values"
      ],
      "metadata": {
        "id": "JMep6CkWAxnY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's see what's in the home.dest columns\n",
        "\n",
        "df[\"home.dest\"]\n",
        "\n",
        "# df[\"home.dest\"].?"
      ],
      "metadata": {
        "id": "S3xjBLU_LJcN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# That's a lot of values... \n",
        "# let's check the age column\n",
        "\n",
        "df['age'].?\n",
        "# Don't ask why some ages are really specific decimals, I don't know either"
      ],
      "metadata": {
        "id": "CRqPeK7HLuMf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "For columns where we know the values already, it's more helpful to see the distribution. Specifically, how could we see what the most common ages were?\n",
        "\n",
        "### Do it yourself!\n",
        "\n",
        "There's a way to summarize the column to show the age and the number of times that age appears in the data. Look around for a function that lets you summarize"
      ],
      "metadata": {
        "id": "FIfQUQsCGx6A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# It should sort by count descending, and if you look at the length, it tells you the number of unique values too :)\n",
        "df['age'].?\n"
      ],
      "metadata": {
        "id": "GwEIipnsM2EJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# and let's check embarked to see where people came from. Use the same function as the cell above!\n",
        "\n",
        "df['embarked'].?"
      ],
      "metadata": {
        "id": "WrUxULpJNXDP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "There are tons of Pandas operations you can use to look into data. Feel free to try out some of the other options\n",
        "\n",
        "For example, what does this one below do?"
      ],
      "metadata": {
        "id": "cgS3-85ZHuRv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df[[\"sex\", \"age\"]].groupby(\"sex\").describe()"
      ],
      "metadata": {
        "id": "7S3BNU2GHm9x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Cleaning, Feature Analysis and Feature Selection\n",
        "---\n",
        "Alright, let's get into prepping the data for some ML! This will go over a few different subjects all at once (for the sake of time). As you can probably guess, we don't need to clean features we don't plan on using, so we'll select some features to use first, then clean those\n",
        "\n",
        "Also, when you see the word \"feature\" here, let's internally convert that to \"column\" when talking about the data. \"Feature\" is a generic term that covers all types of machine learning.\n"
      ],
      "metadata": {
        "id": "ljygcpN7Anid"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Selecting features\n",
        "\n",
        "We are going to be selecting a handful of features by simply removing the features we don't plan on using. We'll also be removing features that are obviously giving us the answer to our question\n",
        "\n",
        "### Food for thought\n",
        "\n",
        "1. What columns probably don't affect the outcome too much?\n",
        "2. Which columns tell you the \"same\" information?\n",
        "3. Which columns will be hard to use?"
      ],
      "metadata": {
        "id": "4KStGdIAHHrM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's remove some features. To simplify the deletion, we're going to provide the columns as a list, then drop them all at once.\n",
        "\n",
        "If you make changes to this list, the steps below may change depending on what features you select.\n",
        "\n",
        "If you delete columns and want them back, run the cell that loads the dataframe from your google drive again to bring back all the columns. Just know that you'll have to run your data cleaning again!\n",
        "\n",
        "To do this easily, click on the cell below, then go to `runtime` -> `run before` to run all the cells up to this point. It may make you connect your google drive again, so feel free to comment out the drive.mount line if that popup gets annoying"
      ],
      "metadata": {
        "id": "jO_P37SLHkLc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "columns_to_remove = [\"name\", \"ticket\", \"boat\", \"body\", \"home.dest\", \"sibsp\", \"parch\", \"cabin\"]\n",
        "\n",
        "df = df.drop(columns_to_remove, axis=1)\n",
        "\n",
        "df"
      ],
      "metadata": {
        "id": "Ne0VnKZBHjZM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Encoding features\n",
        "\n",
        "For features like embarked and sex, words are not easy for machine learning models to use. Their values are also in an enumerated list we already have. These features are known as categorical features. \n",
        "Let's encode those by assigning a number per category, and replacing it"
      ],
      "metadata": {
        "id": "wUZxQemkPRUz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# to convert a categoricl variable, switch the type to category, and then grab the code per\n",
        "df[\"sex\"]=df[\"sex\"].astype('category').cat.codes\n",
        "\n",
        "df"
      ],
      "metadata": {
        "id": "PfE8zdZHQA0A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's do the same for embarked \n",
        "df[\"embarked\"]=?\n",
        "\n",
        "df"
      ],
      "metadata": {
        "id": "JRqZ9RbTRDht"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Removing those pesky null values\n"
      ],
      "metadata": {
        "id": "DHpZfBRhB73W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's double back around and check where we are on those null values\n",
        "\n",
        "df.isnull().sum()"
      ],
      "metadata": {
        "id": "OlsY1aBAB5da"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "A few are still there, and while we could take the time to try and work through that, we are just going to drop those rows for simplicitiy.\n",
        "\n",
        "Notice the number of rows go down below the dataframe"
      ],
      "metadata": {
        "id": "EiRM1VOHO2vw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# dropna drops a row if there is a single column with a null values in it\n",
        "df = df.dropna()\n",
        "\n",
        "df"
      ],
      "metadata": {
        "id": "qkHm014vO1DS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Machine Learning!"
      ],
      "metadata": {
        "id": "Cel-ubQKRQGR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Prepping the data\n",
        "\n",
        "To finally prep the data, we need to split it up into test data, and training data\n",
        "\n"
      ],
      "metadata": {
        "id": "0m6qIHHUR1qD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's break apart the column we're predicting and the columns we'll be using to make the prediction\n",
        "\n",
        "# pop takes the single column out of the dataframe and returns it, so it makes our job pretty easy\n",
        "target = df.pop(\"survived\")\n",
        "\n",
        "target"
      ],
      "metadata": {
        "id": "eLFqDvPMRPQZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# as you can see the dataframe is missing the column\n",
        "\n",
        "df"
      ],
      "metadata": {
        "id": "kZJWgxnqStAM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# We're going to use sklearn to speed up a lot of the code here, so we're going to split the data into test data and training data in one line\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# This saves the output into 4 variables, and the param test_size controls the percentage of values in the test dataset vs the training dataset\n",
        "x_train, x_test, y_train, y_test = train_test_split(df, target, test_size=0.2, random_state=42)"
      ],
      "metadata": {
        "id": "q-Bei9bWTAPy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Running the algorithm!"
      ],
      "metadata": {
        "id": "PzF4VXWvT9zl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "model = SVC(kernel='linear', random_state=42)\n",
        "\n",
        "model.fit(x_train, y_train)\n",
        "\n",
        "y_pred = model.predict(x_test)\n",
        "\n",
        "print('Accuracy: %.3f' % accuracy_score(y_test, y_pred))"
      ],
      "metadata": {
        "id": "SyIBzrhxTzx9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Boom! You've done machine learning!"
      ],
      "metadata": {
        "id": "jC0S8-c-WcVO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Some other algorithms\n",
        "\n",
        "Let's try out some other options in the library and see how they do!"
      ],
      "metadata": {
        "id": "k-rBSeMjTRxP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Naive Bayes\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "\n",
        "model = GaussianNB()\n",
        "\n",
        "model.fit(x_train, y_train)\n",
        "\n",
        "y_pred = model.predict(x_test)\n",
        "\n",
        "print('Accuracy: %.3f' % accuracy_score(y_test, y_pred))"
      ],
      "metadata": {
        "id": "q9_TszkocivX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Decision Trees\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "\n",
        "model = DecisionTreeClassifier(random_state=42)\n",
        "\n",
        "model.fit(x_train, y_train)\n",
        "\n",
        "y_pred = model.predict(x_test)\n",
        "\n",
        "print('Accuracy: %.3f' % accuracy_score(y_test, y_pred))"
      ],
      "metadata": {
        "id": "KH4BMOlZSf-I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This one has an extra twist. We can also visualize this one quite easily to see what the final decision tree looks like.\n",
        "\n",
        "Run the cell below, and look at the output. Can you figure out what the values in each box mean?"
      ],
      "metadata": {
        "id": "nLEHRMoQTrxP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from graphviz import Source\n",
        "from sklearn import tree\n",
        "\n",
        "Source(tree.export_graphviz(model, out_file=None, feature_names=x_train.columns.values))"
      ],
      "metadata": {
        "id": "zvGQh7WVTni6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Writing less code\n",
        "\n",
        "Before we begin, let's reduce the amount of code we are writing for our models.\n",
        "\n",
        "Let's make a function that we can use to test functions a bit quicker. \n",
        "\n",
        "Look for a pattern in the Decision Tree and SVM classifiers , and make a function that replaces the duplicate code"
      ],
      "metadata": {
        "id": "5giRHjJvUSIq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_and_test_model(model, x_train, x_test, y_train, y_test):\n",
        "  "
      ],
      "metadata": {
        "id": "2NmH7KiHVEX2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now lets use that to try out the random forest classifier"
      ],
      "metadata": {
        "id": "WnmrXlRdJpYO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "model = RandomForestClassifier(random_state=42)\n",
        "\n",
        "train_and_test_model(model, x_train, x_test, y_train, y_test)"
      ],
      "metadata": {
        "id": "z0Eyj9JcTy-N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Tuning hyperparameters\n",
        "\n",
        "While just hitting run and seeing how algorithms do is fun, let's spice things up a bit\n",
        "\n",
        "This next algorithm will be K-nearest neighbors, and you'll get to see how changing the k value affects the accuracy"
      ],
      "metadata": {
        "id": "3h2b2IeiUh9c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "\n",
        "model = KNeighborsClassifier(n_neighbors=1)\n",
        "\n",
        "train_and_test_model(model, x_train, x_test, y_train, y_test)"
      ],
      "metadata": {
        "id": "NOBEoe6HUzRE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Automating busy work\n",
        "\n",
        "We could type every single n value into this variable until a good one shows up...\n",
        "\n",
        "But we can just do that with loops!"
      ],
      "metadata": {
        "id": "htJo2SPgXlqt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Do it yourself!\n",
        "Add a loop in and see which value for n_neighbors performs the best! Print out the number of neighbors, then call the function you already wrote. \n",
        "\n",
        "Try out 1-25, and see which one performs the best!"
      ],
      "metadata": {
        "id": "PRzWjln_JyAH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# \n",
        "for ???:\n",
        "  model = ?\n",
        "  \n",
        "  print(?)"
      ],
      "metadata": {
        "id": "MJBfkUP4X3vo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Bonus work!\n",
        "\n",
        "Try going back and adjusting the features you select, or removing cleaning steps by commenting out code. If you're feeling fun, even try engineering your own features to use! You can run all the code in the notebook by going to `runtime` -> `run all` to run everything again after making changes. \n",
        "\n",
        "Can you improve the accuracy? Do certain models perform better than others when you make certain changes?"
      ],
      "metadata": {
        "id": "3nk6EAfBJ6ea"
      }
    }
  ]
}